# agentic_rag_pipeline/components/chunker.py (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô V2 + V5)

import re
from typing import List, Dict, Any, Optional
from langchain.text_splitter import RecursiveCharacterTextSplitter
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.core.schema import Document

from agentic_rag_pipeline.core.llm_provider import get_embed_model 


# --- [V5+V2] ‡πÄ‡∏û‡∏¥‡πà‡∏° Import ‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡πÑ‡∏õ ---
from llama_index.embeddings.huggingface import HuggingFaceEmbedding # <-- ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß
from agentic_rag_pipeline import config


# --- [V2] ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î Helper Function 1: Recursive ---
def _recursive_strategy(
    text_piece: str, 
    base_metadata: dict, 
    start_chunk_num: int,  # <-- [V2] ‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
    chunk_size: int = 1000,
    chunk_overlap: int = 150
) -> List[Dict[str, Any]]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (RecursiveCharacterTextSplitter)
    """
    print(f" -> ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Recursive Splitting (Size: {chunk_size}, Overlap: {chunk_overlap})...")
    
    separators = ["\\n\\n", "\\n", " ", ""]
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=separators
    )
    
    split_texts = text_splitter.split_text(text_piece)
    
    chunks = []
    doc_title = base_metadata.get("document_title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
    section_title = base_metadata.get("section_title", "N/A") # <-- [V2] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ Section

    for i, text in enumerate(split_texts):
        chunk_metadata = base_metadata.copy()
        chunk_metadata["chunk_number"] = start_chunk_num + i # <-- [V2] ‡∏ô‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ï‡πà‡∏≠
        
        # [V2] ‡πÄ‡∏û‡∏¥‡πà‡∏° Context ‡∏Ç‡∏≠‡∏á Section ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
        enriched_content = f"‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {doc_title}\n‡∏™‡πà‡∏ß‡∏ô: {section_title}\n\n{text}"
        
        chunks.append({
            "content": enriched_content,
            "metadata": chunk_metadata
        })
        
    return chunks

# --- [V2] ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î Helper Function 2: Structural ---
def _structural_strategy(
    text_piece: str, 
    base_metadata: dict,
    start_chunk_num: int  # <-- [V2] ‡∏£‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
) -> List[Dict[str, Any]]:
    """
    ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏ä‡πà‡∏ô '‡∏°‡∏≤‡∏ï‡∏£‡∏≤', '‡∏ö‡∏ó‡∏ó‡∏µ‡πà', '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:'
    """
    print(" -> ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Structural Splitting...")
    
    patterns = [
        r'(\\n‡∏°‡∏≤‡∏ï‡∏£‡∏≤\\s+\\d+)',     # ‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢
        r'(\\n‡∏ö‡∏ó‡∏ó‡∏µ‡πà\\s+\\d+)',      # ‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö/‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠
        r'(\\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:)',          # ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ Q&A
    ]
    
    final_chunks = []
    
    for pattern in patterns:
        if re.search(pattern, text_piece):
            print(f" -> ‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á! ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏° pattern: {pattern}")
            
            structural_parts = re.split(pattern, text_piece)
            
            combined_parts = []
            if structural_parts[0] and structural_parts[0].strip():
                 combined_parts.append(structural_parts[0].strip())

            for i in range(1, len(structural_parts), 2):
                combined_chunk = (structural_parts[i] + structural_parts[i+1]).strip()
                combined_parts.append(combined_chunk)
            
            # [V2] ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ Recursive ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏¥‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÉ‡∏´‡∏ç‡πà‡∏≠‡∏¢‡∏π‡πà
            # ‡πÇ‡∏î‡∏¢‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠ global_chunk_counter
            global_chunk_counter = start_chunk_num
            for part in combined_parts:
                sub_chunks = _recursive_strategy(
                    text_piece=part,
                    base_metadata=base_metadata,
                    start_chunk_num=global_chunk_counter, # <-- [V2] ‡∏™‡πà‡∏á‡πÄ‡∏•‡∏Ç‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
                    chunk_size=1000, # (‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ Default ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sub-chunking)
                    chunk_overlap=150
                )
                final_chunks.extend(sub_chunks)
                global_chunk_counter += len(sub_chunks) # <-- [V2] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö
            
            return final_chunks 

    print(" -> ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
    return []


# --- [V2] ‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î Helper Function 3: Semantic ---
def _semantic_strategy(
    text_piece: str,
    base_metadata: dict,
    start_chunk_num: int,
    breakpoint_threshold: int = 95 
) -> List[Dict[str, Any]]:
    print(f" -> ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå Semantic Splitting (Threshold: {breakpoint_threshold})...")
    try:
        # [‡πÉ‡∏´‡∏°‡πà!] ‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding Wrapper ‡∏Ç‡∏≠‡∏á LlamaIndex ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        wrapped_embed_model = HuggingFaceEmbedding(model_name=config.EMBED_MODEL_NAME)

        splitter = SemanticSplitterNodeParser(
            embed_model=wrapped_embed_model, # <--- ‡∏™‡πà‡∏á Wrapper ‡∏Ç‡∏≠‡∏á LlamaIndex ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            breakpoint_percentile_threshold=breakpoint_threshold
        )
        nodes = splitter.get_nodes_from_documents([Document(text=text_piece)])

        chunks = []
        doc_title = base_metadata.get("document_title", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
        section_title = base_metadata.get("section_title", "N/A") # <-- [V2] ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠ Section

        for i, node in enumerate(nodes):
            chunk_metadata = base_metadata.copy()
            chunk_metadata["chunk_number"] = start_chunk_num + i # <-- [V2] ‡∏ô‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏ï‡πà‡∏≠
            
            # [V2] ‡πÄ‡∏û‡∏¥‡πà‡∏° Context ‡∏Ç‡∏≠‡∏á Section ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            enriched_content = f"‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: {doc_title}\n‡∏™‡πà‡∏ß‡∏ô: {section_title}\n\n{node.get_content()}"
            chunks.append({"content": enriched_content, "metadata": chunk_metadata})

        return chunks
    except Exception as e:
        print(f"   -> ‚ùå Semantic Splitting ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        return [] # ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏•‡∏¥‡∏™‡∏ï‡πå‡∏ß‡πà‡∏≤‡∏á

# --- [V2+V5] Main Function (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î) ---
def create_chunks_for_text(
    text: str,
    metadata: Dict[str, Any],
    original_filename: str,
    layout_map: Dict[str, Any],         # <-- [V2] ‡∏£‡∏±‡∏ö "‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á"
    retry_instructions: Dict[str, Any]  # <-- [V5] ‡∏£‡∏±‡∏ö "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏Å‡πâ"
) -> List[Dict[str, Any]]:
    
    print(f"‡∏™‡∏ñ‡∏≤‡∏ô‡∏µ‡∏ó‡∏µ‡πà 3: Agent Chunker (V2) ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ï‡∏≤‡∏° '‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á'...")
    
    all_chunks = []
    global_chunk_counter = 1
    
    sections = layout_map.get("sections", [])
    
    # --- [V2] Fallback ‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ "‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á" (Layout Map) ---
    if not sections:
        print("   -> ‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö '‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á', ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Recursive ‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
        sections = [{
            "section_id": 1, 
            "title": "Full Document", 
            "char_start": 0, 
            "char_end": len(text),
            "recommended_strategy": "recursive"
        }]

    # --- [V2] ‡∏ß‡∏ô Loop ‡∏ï‡∏≤‡∏° "‡πÅ‡∏ú‡∏ô‡∏ú‡∏±‡∏á" (Layout Map) ---
    for section in sections:
        section_id = section.get("section_id")
        title = section.get("title", "N/A")
        start = section.get("char_start", 0)
        end = section.get("char_end", len(text))
        strategy = section.get("recommended_strategy", "recursive")

        # --- [V5] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏Å‡πâ" ‡∏à‡∏≤‡∏Å Validator ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Section ‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ---
        if retry_instructions and retry_instructions.get("target_section_id") == section_id:
            new_strategy = retry_instructions.get("suggestion", strategy)
            print(f"   -> üí° V5: ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏Å‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Section '{title}', ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏à‡∏≤‡∏Å '{strategy}' ‡πÄ‡∏õ‡πá‡∏ô '{new_strategy}'")
            strategy = new_strategy # <-- ‡πÉ‡∏ä‡πâ‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡πÉ‡∏´‡∏°‡πà‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà "‡πÅ‡∏û‡∏ó‡∏¢‡πå" ‡∏™‡∏±‡πà‡∏á

        print(f"   -> ‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• Section: '{title}' (‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå: {strategy.upper()})")
        
        # ‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô (Section Text)
        section_text = text[start:end]
        if not section_text.strip():
            print(f"   -> ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° Section '{title}' ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤")
            continue
            
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Metadata ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chunks ‡πÉ‡∏ô Section ‡∏ô‡∏µ‡πâ
        section_metadata = metadata.copy()
        section_metadata["section_id"] = section_id
        section_metadata["section_title"] = title
        section_metadata["strategy_used"] = strategy

        section_chunks = []
        
        # --- [V2] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î ---
        if strategy == "structural":
            section_chunks = _structural_strategy(section_text, section_metadata, global_chunk_counter)
            # [V2] Fallback
            if not section_chunks:
                print("   -> ‚ö†Ô∏è Structural ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡πÉ‡∏ä‡πâ Recursive ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á")
                section_chunks = _recursive_strategy(section_text, section_metadata, global_chunk_counter)
        
        elif strategy == "semantic":
            section_chunks = _semantic_strategy(section_text, section_metadata, global_chunk_counter)
            # [V2] Fallback
            if not section_chunks:
                print("   -> ‚ö†Ô∏è Semantic ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡πÉ‡∏ä‡πâ Recursive ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á")
                section_chunks = _recursive_strategy(section_text, section_metadata, global_chunk_counter)
        
        else: # Default to "recursive"
            section_chunks = _recursive_strategy(section_text, section_metadata, global_chunk_counter)
            
        all_chunks.extend(section_chunks)
        global_chunk_counter += len(section_chunks) # <-- [V2] ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏ï‡∏±‡∏ß‡∏ô‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Section ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

    print(f"   -> ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á Chunks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_chunks)} ‡∏ä‡∏¥‡πâ‡∏ô ‡∏à‡∏≤‡∏Å {len(sections)} ‡∏™‡πà‡∏ß‡∏ô")
    return all_chunks